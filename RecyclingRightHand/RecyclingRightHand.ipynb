{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Pip installing tools required in order to import the dataset"
      ],
      "metadata": {
        "id": "i1iQoVIu7CAm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSRaXmVh3qTI",
        "outputId": "c916d7ab-867c-4eff-b038-bb3049b12bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries required for installing the dataset and downloaded the dataset\n",
        "\n",
        "* Kaggle Username: Veronica Mordvinova2\n",
        "* Kaggle Key: will be given during the lesson"
      ],
      "metadata": {
        "id": "APPrPaY68UIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "\t\"https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450thkUd7BL9",
        "outputId": "16d2841d-bf77-4691-9b9d-bc9149f9bc87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: Veronica Mordvinova2\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\n",
            "Downloading garbage-classification.zip to ./garbage-classification\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 82.0M/82.0M [00:02<00:00, 32.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing and Formatting the Data**"
      ],
      "metadata": {
        "id": "iDRX3lzmG6_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the rest of the libraries needed to train the model"
      ],
      "metadata": {
        "id": "I6Wvs2S_F6H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "iF7ju95pGBtX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading in the images from the dataset and splitting them into \"training\" and \"validation\" data\n",
        "\n",
        "Training data - is labeled data which we give the computer in order for it to learn how to guess images correctly\n",
        "\n",
        "Validation data - used to see how accurate the computer's guesses are"
      ],
      "metadata": {
        "id": "XwPaAp3hGMkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"garbage-classification/Garbage classification/Garbage classification\",\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(256, 256),\n",
        "  batch_size=5)\n",
        "\n",
        "validation_data = tf.keras.utils.image_dataset_from_directory(\n",
        "  \"garbage-classification/Garbage classification/Garbage classification\",\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(256, 256),\n",
        "  batch_size=5)\n",
        "\n",
        "class_names = training_data.class_names\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdglGSuNGUXv",
        "outputId": "988bbaa0-91a1-477b-9fec-5469f9ca1ade"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2527 files belonging to 6 classes.\n",
            "Using 2022 files for training.\n",
            "Found 2527 files belonging to 6 classes.\n",
            "Using 505 files for validation.\n",
            "['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Building the Model**\n"
      ],
      "metadata": {
        "id": "6Lasv-7-GxHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure that there is buffered prefetching - so you can you can yield date from disk without having I/O becoming a blocker\n",
        "\n",
        "Dataset.cache - allow the images to be stored in memory so they don't have to be searched for repeatedly with each iteration\n",
        "\n",
        "Dataset.prefetch - connects the cache to the model execution (you'll see this further down on the page)"
      ],
      "metadata": {
        "id": "YI64yer3HL28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "training_data = training_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "validation_data = validation_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "xKAnUnaWew0B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since colours appear as values between 0 and 255, and we want our values to be in the 0-1 range - we will divide the numbers by 255 in order to **normalize** them."
      ],
      "metadata": {
        "id": "GlVVhWGufndx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)"
      ],
      "metadata": {
        "id": "gkLM3tvg6Af6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we apply the **\"normalization\"** - meaning that we are dividing the pixel values in each image, and making sure that the computer is still able to know what the image is of."
      ],
      "metadata": {
        "id": "VjbXMgdWf8Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_ds = training_data.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "# Notice the pixel values are now in `[0,1]`.\n",
        "print(np.min(first_image), np.max(first_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsZlFCBLe7Kj",
        "outputId": "c865c5e5-c6ec-4a54-a8a7-ddb4b9917328"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.21617648 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are building what you would see in a machine learning diagram - a lot of different layers that connect together.\n",
        "\n",
        "These layers filter information and allow the machine to learn!"
      ],
      "metadata": {
        "id": "RaZ3uk1xHurf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Rescaling(1./255),\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\", input_shape=(256,\n",
        "  256,3)),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "  tf.keras.layers.RandomZoom(0.1),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_classes)\n",
        "])"
      ],
      "metadata": {
        "id": "WbJvedWPHx0R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are compiling (just like in Python) our model, so that we can see a variety of summary information (such as accuracy of the model at any step) when we are training our model."
      ],
      "metadata": {
        "id": "CcWdMfM5HXoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1zjSk0pDHhqK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training the Model**"
      ],
      "metadata": {
        "id": "slk7qBmxH3bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train the model! We are using our training data to train the model and the validation data in order to test how accurate the machine's guesses are\n",
        "\n",
        "Epochs are how many passthroughs of the data that you want to do, in order to make the model more accurate."
      ],
      "metadata": {
        "id": "Mm7RQyRJhWbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "  training_data,\n",
        "  validation_data=validation_data,\n",
        "  epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "7kPqoEz6tMVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd2ff1d-d1b5-43fb-db3c-82f474a57214"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3547 - accuracy: 0.8788 - val_loss: 1.0222 - val_accuracy: 0.7168\n",
            "Epoch 2/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.4061 - accuracy: 0.8581 - val_loss: 1.0470 - val_accuracy: 0.7248\n",
            "Epoch 3/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3612 - accuracy: 0.8783 - val_loss: 0.9594 - val_accuracy: 0.7208\n",
            "Epoch 4/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3518 - accuracy: 0.8759 - val_loss: 1.0176 - val_accuracy: 0.7149\n",
            "Epoch 5/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3548 - accuracy: 0.8843 - val_loss: 1.0774 - val_accuracy: 0.7089\n",
            "Epoch 6/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3079 - accuracy: 0.8917 - val_loss: 1.0446 - val_accuracy: 0.7050\n",
            "Epoch 7/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3153 - accuracy: 0.8922 - val_loss: 0.9848 - val_accuracy: 0.7307\n",
            "Epoch 8/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3331 - accuracy: 0.8769 - val_loss: 1.2780 - val_accuracy: 0.6990\n",
            "Epoch 9/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3329 - accuracy: 0.8947 - val_loss: 1.1513 - val_accuracy: 0.7089\n",
            "Epoch 10/10\n",
            "405/405 [==============================] - 3s 8ms/step - loss: 0.3288 - accuracy: 0.8932 - val_loss: 1.3865 - val_accuracy: 0.6653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Testing**"
      ],
      "metadata": {
        "id": "8CSNv8-oUz1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are you curious as to how accurate our models ended up being?\n",
        "\n",
        "You can test our machine learning model by inserting a new link in the \"test_url\" variable and seeing if it predicted the result accurately!\n",
        "\n",
        "Follow up question: why might it not have predicted the material accurately?"
      ],
      "metadata": {
        "id": "XGhd56wIiJJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = \"https://images-na.ssl-images-amazon.com/images/I/81VQ-mOl7CL.jpg\"\n",
        "test_path = tf.keras.utils.get_file(origin=test_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    test_path, target_size=(256, 256)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vSB8OckU5hI",
        "outputId": "e87c22e8-ec47-4a83-db7f-78e9137dcf46"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "This image most likely belongs to cardboard with a 99.05 percent confidence.\n"
          ]
        }
      ]
    }
  ]
}